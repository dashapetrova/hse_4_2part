{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "assignment_1_bpe.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke6JHBnnGkH_",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize text by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWRhMauVIedY",
        "colab_type": "code",
        "outputId": "c7671c60-0d5a-4970-fac0-da16cfda7140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9l11XaMIq4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwA95MJxGkIO",
        "colab_type": "code",
        "outputId": "b36bab4f-5dc9-46ff-9271-88812bfc4f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open('peace.txt', 'r').read()[2:]\n",
        "len(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3227579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0fP5LWCGkIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # TODO\n",
        "    # make lowercase\n",
        "    # replace all punctuation except dots with spaces\n",
        "    text = re.sub('~|`|!|\"|@|#|\\$|%|&|\\'|”|“|—|‘|’|\\(|\\)|\\\\|^|_|{|\\||}|\\*|\\+|,|-|/|:|;|<|=|>|\\?|\\]|\\[', ' ', text).lower()\n",
        "    # collapse multiple spaces into one '   ' -> ' '\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "text = preprocess_text(text)\n",
        "assert len(text) == 3141169"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCtEO25zGkJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26a9ojb7QCuS",
        "colab_type": "code",
        "outputId": "ae9e2db0-a68e-4fb7-fcd0-9b684c7165f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the project gutenberg ebook of war and peace by leo tolstoy this ebook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "S_yumW8EGkJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "from sklearn.base import TransformerMixin\n",
        "import copy\n",
        "\n",
        "\n",
        "class BPE(TransformerMixin):\n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "        \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        fit itos and stoi\n",
        "        text: list of strings \n",
        "        \"\"\"\n",
        "        \n",
        "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
        "        \n",
        "        all_uniq_letters = list(set(''.join(text)))\n",
        "        self.itos = all_uniq_letters\n",
        "        \n",
        "        d = {}\n",
        "        for i in range(len(all_uniq_letters)):\n",
        "          letter = all_uniq_letters[i]\n",
        "          d[letter] = i\n",
        "        self.stoi = d\n",
        "        \n",
        "        mas = []\n",
        "        for i in range(len(text)):\n",
        "          mas_dop = []\n",
        "          for j in text[i]:\n",
        "            mas_dop.append(self.stoi[j])\n",
        "          mas.append(mas_dop)\n",
        "        text = mas\n",
        "        \n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            # TODO\n",
        "            # count bigram freqencies in the text\n",
        "            bigrams = Counter()\n",
        "            for i in range(len(text)):\n",
        "              bigrams.update((x, y) for x, y in zip(*[text[i][j:] for j in range(2)]))\n",
        "            \n",
        "            top = bigrams.most_common(1)\n",
        "            new_token = top[0][0]# most common bigram in the text\n",
        "            new_id = len(self.itos)\n",
        "            \n",
        "            self.itos.append(new_token)\n",
        "            self.stoi[new_token] = new_id\n",
        "            \n",
        "            # find occurences of the new_token in the text and replace them with new_id\n",
        "            new_text = text\n",
        "            for i, s in enumerate(text):\n",
        "              k = 0\n",
        "              for j, (q, p) in enumerate(zip(s[:-1], s[1:])):\n",
        "                if (q, p) == new_token:\n",
        "                  new_text[i][j-k] = new_id\n",
        "                  del new_text[i][j-k+1]\n",
        "                  k += 1\n",
        "            text = new_text\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        convert text to a sequence of token ids\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        new_text = []\n",
        "        for sent in text:\n",
        "          ms_dop = []\n",
        "          for symbol in sent:\n",
        "            if symbol in self.itos:\n",
        "              symbol = self.stoi[symbol]\n",
        "              ms_dop.append(symbol)\n",
        "          new_text.append(ms_dop)\n",
        "        text = new_text\n",
        "        \n",
        "        for token_id, token in enumerate(self.itos):\n",
        "          new_text_2 = text\n",
        "          for i, s in enumerate(text):\n",
        "            k = 0\n",
        "            for j, (q, p) in enumerate(zip(s[:-1], s[1:])):\n",
        "              if (q, p) == token:\n",
        "                new_text_2[i][j-k] = token_id\n",
        "                del new_text_2[i][j-k+1]\n",
        "                k += 1\n",
        "          text = new_text_2\n",
        "          \n",
        "        return text\n",
        "    \n",
        "    def decode_token(self, tok):\n",
        "        \"\"\"\n",
        "        tok: int or tuple\n",
        "        \"\"\"\n",
        "        \n",
        "        def rec_func(token):\n",
        "          if type(token) == int:\n",
        "            content = self.itos[token]\n",
        "            if type(content) == str:\n",
        "              result = content\n",
        "            else:\n",
        "              result = rec_func(content)\n",
        "          if type(token) == tuple:\n",
        "            result = rec_func(token[0]) + rec_func(token[1])\n",
        "          return result\n",
        "        \n",
        "        result = rec_func(tok)\n",
        "        return result\n",
        "            \n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))\n",
        "        \n",
        "        \n",
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4wCCRgVfSbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_uniq_letters_TEST = list(set(''.join(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJ9HxBRfkug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d_TEST = {}\n",
        "for i in range(len(all_uniq_letters_TEST)):\n",
        "  letter = all_uniq_letters_TEST[i]\n",
        "  d_TEST[letter] = i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWOP5aK2fyK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mas_TEST = []\n",
        "for i in range(len(text)):\n",
        "  mas_dop = []\n",
        "  for j in text[i]:\n",
        "    mas_dop.append(d_TEST[j])\n",
        "  mas_TEST.append(mas_dop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpXXXnfQgFw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#mas_TEST[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bq3EmqegSNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams_TEST = Counter()\n",
        "for i in range(len(text)):\n",
        "  bigrams_TEST.update((x, y) for x, y in zip(*[text[i][j:] for j in range(2)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRRtusu4geYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bigrams_TEST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtCC1QNQGkJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == text[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-gRNEtdGkJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "        \n",
        "    \n",
        "start_token = vocab_size\n",
        "end_token = vocab_size + 1\n",
        "        \n",
        "    \n",
        "class LM:\n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.proba = Counter()# TODO create array for storing 3-gram counters\n",
        "        \n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        m = []\n",
        "        for i in range(self.vocab_size):\n",
        "          m.append(self.get_proba(a,b,i))\n",
        "        result = m\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "\n",
        "        delta = 1\n",
        "        smooth = (self.proba[(a, b, c)] + delta) ** (1/tau)\n",
        "        \n",
        "        mas  = []\n",
        "        for i in range(self.vocab_size):\n",
        "          mas.append((self.proba[(a, b, i)] + delta) ** (1/tau))\n",
        "        \n",
        "        result = smooth / sum(mas) # TODO approximate probability by counters\n",
        "        \n",
        "        return result\n",
        "    \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        train language model on text\n",
        "        text: list of lists\n",
        "        \"\"\"\n",
        "        ms = []\n",
        "        for t in text:\n",
        "          s = [start_token] + t + [end_token]\n",
        "          ms.append(s)\n",
        "          \n",
        "        trigrams = Counter()\n",
        "        for i in range(len(ms)):\n",
        "          trigrams.update((x, y, z) for x, y, z in zip(*[ms[i][j:] for j in range(3)]))\n",
        "        \n",
        "        self.proba = trigrams# TODO count 3-grams in the text\n",
        "        \n",
        "        return self\n",
        "    \n",
        "lm = LM(vocab_size, 1).fit(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHo_zyQ8GkJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from math import log\n",
        "\n",
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate sequence from language model *lm* conditioned on input_seq\n",
        "    input_seq: sequence of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated sequence length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    \n",
        "    input_seq_new = [start_token] + input_seq\n",
        "    x, y = input_seq_new[-2:]\n",
        "    beam = [] # TODO store in beam tuples of current sequences and their log probabilities\n",
        "    m = lm.infer(x, y, tau)\n",
        "    for token, proba in sorted(enumerate(m), key=lambda x:x[1], reverse=True)[:k]:\n",
        "      beam.append(([y, token], proba))\n",
        "    \n",
        "    for i in range(max_len):\n",
        "        candidates = []\n",
        "        candidates_proba = []\n",
        "        for snt, snt_proba in beam:\n",
        "            if snt[-1] == end_token: # TODO process terminal token\n",
        "              continue\n",
        "            else:\n",
        "              a, b = snt[-2:]\n",
        "              proba = lm.infer(a, b, tau) # probability vector of the next token\n",
        "              best_k = sorted(enumerate(proba), key=lambda x:x[1], reverse=True)[:k] # top-k most probable tokens\n",
        "              \n",
        "              # TODO update candidates' sequences and corresponding probabilities\n",
        "              mas_dop_1 = []\n",
        "              mas_dop_2 = []\n",
        "              for token, proba in best_k:\n",
        "                mas_dop_1.append(snt+[token])\n",
        "                mas_dop_2.append(snt_proba + log(proba))\n",
        "              candidates += mas_dop_1\n",
        "              candidates_proba += mas_dop_2\n",
        "        \n",
        "        beam = []# select top-k most probable sequences from candidates\n",
        "        for idx, prob in sorted(enumerate(candidates_proba), key=lambda x:x[1], reverse=True):\n",
        "            beam.append((candidates[idx], proba))\n",
        "    result = []\n",
        "    for i, prob in beam:\n",
        "      result.append((i[1:], exp(prob)))\n",
        "    return result\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N69jrbdd8rm",
        "colab_type": "text"
      },
      "source": [
        " Следующие блоки с примерами долго работали, так ничего и не удалось вывести"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47lwM1jQd9Tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for sent, proba in result:\n",
        "  print('sent: ', bpe.decode(sent))\n",
        "  print('probability: ', proba, '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCP9qvTaGkJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for sent, proba in result:\n",
        "  print('sent: ', bpe.decode(sent))\n",
        "  print('probability: ', proba, '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA9LMmrEGkKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for sent, proba in result:\n",
        "  print('sent: ', bpe.decode(sent))\n",
        "  print('probability: ', proba, '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jrOHW_9GkKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for sent, proba in result:\n",
        "  print('sent: ', bpe.decode(sent))\n",
        "  print('probability: ', proba, '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}