{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "assignment _5_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QSKsxv7uQsKH"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuXNFgk-_auf",
        "colab_type": "text"
      },
      "source": [
        "**Assignment 5**\n",
        "\n",
        "Build CNN model for sentiment analysis (binary classification) of IMDB Reviews (https://www.kaggle.com/utathya/imdb-review-dataset). You can use data with label=\"unsup\" for pretraining of embeddings. Here you are forbidden to use test dataset for pretraining of embeddings.\n",
        "\n",
        "Your quality metric is accuracy score on test dataset. Look at \"type\" column for train/test split.\n",
        "\n",
        "You can use pretrained embeddings from external sources.\n",
        "\n",
        "You have to provide data for trials with different hyperparameter values.\n",
        "\n",
        "You have to beat following baselines:\n",
        "\n",
        "[3 points] acc = 0.75\n",
        "\n",
        "[5 points] acc = 0.8\n",
        "\n",
        "[8 points] acc = 0.9\n",
        "\n",
        "[2 points] for using unsupervised data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmmLaYAww1Cq",
        "colab_type": "code",
        "outputId": "9dc19d2f-8df4-4f2c-94fb-6e770e138b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.externals import joblib\n",
        "import nltk\n",
        "import gensim\n",
        "import spacy\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch as tt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.data import Field, LabelField, BucketIterator, TabularDataset, Iterator, Dataset, RawField\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrh9TCdxazH",
        "colab_type": "code",
        "outputId": "f6ea1c34-c3e1-4aa8-e2b0-52733bed1903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQY8_Bm6zjMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ca55de81-1b1b-4ab5-b278-ea093c218dc2",
        "id": "fOCUkb7vxM1W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "!head imdb.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ",type,review,label,file\r\n",
            "0,test,\"Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\",neg,0_2.txt\r\n",
            "1,test,\"This is an example of why the majority of action films are the same. Generic and boring, there's really nothing worth watching here. A complete waste of the then barely-tapped talents of Ice-T and Ice Cube, who've each proven many times over that they are capable of acting, and acting well. Don't bother with this one, go see New Jack City, Ricochet or watch New York Undercover for Ice-T, or Boyz n the Hood, Higher Learning or Friday for Ice Cube and see the real deal. Ice-T's horribly cliched dialogue alone makes this film grate at the teeth, and I'm still wondering what the heck Bill Paxton was doing in this film? And why the heck does he always play the exact same character? From Aliens onward, every film I've seen with Bill Paxton has him playing the exact same irritating character, and at least in Aliens his character died, which made it somewhat gratifying...<br /><br />Overall, this is second-rate action trash. There are countless better films to see, and if you really want to see this one, watch Judgement Night, which is practically a carbon copy but has better acting and a better script. The only thing that made this at all worth watching was a decent hand on the camera - the cinematography was almost refreshing, which comes close to making up for the horrible film itself - but not quite. 4/10.\",neg,10000_4.txt\r\n",
            "2,test,\"First of all I hate those moronic rappers, who could'nt act if they had a gun pressed against their foreheads. All they do is curse and shoot each other and acting like cliché'e version of gangsters.<br /><br />The movie doesn't take more than five minutes to explain what is going on before we're already at the warehouse There is not a single sympathetic character in this movie, except for the homeless guy, who is also the only one with half a brain.<br /><br />Bill Paxton and William Sadler are both hill billies and Sadlers character is just as much a villain as the gangsters. I did'nt like him right from the start.<br /><br />The movie is filled with pointless violence and Walter Hills specialty: people falling through windows with glass flying everywhere. There is pretty much no plot and it is a big problem when you root for no-one. Everybody dies, except from Paxton and the homeless guy and everybody get what they deserve.<br /><br />The only two black people that can act is the homeless guy and the junkie but they're actors by profession, not annoying ugly brain dead rappers.<br /><br />Stay away from this crap and watch 48 hours 1 and 2 instead. At lest they have characters you care about, a sense of humor and nothing but real actors in the cast.\",neg,10001_1.txt\r\n",
            "3,test,\"Not even the Beatles could write songs everyone liked, and although Walter Hill is no mop-top he's second to none when it comes to thought provoking action movies. The nineties came and social platforms were changing in music and film, the emergence of the Rapper turned movie star was in full swing, the acting took a back seat to each man's overpowering regional accent and transparent acting. This was one of the many ice-t movies i saw as a kid and loved, only to watch them later and cringe. Bill Paxton and William Sadler are firemen with basic lives until a burning building tenant about to go up in flames hands over a map with gold implications. I hand it to Walter for quickly and neatly setting up the main characters and location. But i fault everyone involved for turning out Lame-o performances. Ice-t and cube must have been red hot at this time, and while I've enjoyed both their careers as rappers, in my opinion they fell flat in this movie. It's about ninety minutes of one guy ridiculously turning his back on the other guy to the point you find yourself locked in multiple states of disbelief. Now this is a movie, its not a documentary so i wont waste my time recounting all the stupid plot twists in this movie, but there were many, and they led nowhere. I got the feeling watching this that everyone on set was sord of confused and just playing things off the cuff. There are two things i still enjoy about it, one involves a scene with a needle and the other is Sadler's huge 45 pistol. Bottom line this movie is like domino's pizza. Yeah ill eat it if I'm hungry and i don't feel like cooking, But I'm well aware it tastes like crap. 3 stars, meh.\",neg,10002_3.txt\r\n",
            "4,test,\"Brass pictures (movies is not a fitting word for them) really are somewhat brassy. Their alluring visual qualities are reminiscent of expensive high class TV commercials. But unfortunately Brass pictures are feature films with the pretense of wanting to entertain viewers for over two hours! In this they fail miserably, their undeniable, but rather soft and flabby than steamy, erotic qualities non withstanding.<br /><br />Senso '45 is a remake of a film by Luchino Visconti with the same title and Alida Valli and Farley Granger in the lead. The original tells a story of senseless love and lust in and around Venice during the Italian wars of independence. Brass moved the action from the 19th into the 20th century, 1945 to be exact, so there are Mussolini murals, men in black shirts, German uniforms or the tattered garb of the partisans. But it is just window dressing, the historic context is completely negligible.<br /><br />Anna Galiena plays the attractive aristocratic woman who falls for the amoral SS guy who always puts on too much lipstick. She is an attractive, versatile, well trained Italian actress and clearly above the material. Her wide range of facial expressions (signalling boredom, loathing, delight, fear, hate ... and ecstasy) are the best reason to watch this picture and worth two stars. She endures this basically trashy stuff with an astonishing amount of dignity. I wish some really good parts come along for her. She really deserves it.\",neg,10003_3.txt\r\n",
            "5,test,\"A funny thing happened to me while watching \"\"Mosquito\"\": on the one hand, the hero is a deaf-mute and the director is totally unable to make us understand why he does what he does (mutilating mannequins...er, excuse me, corpses) through his images. On the other hand, the English version at least is very badly dubbed. So I found myself wishing there had been both more AND less dialogue at the same time! This film is stupid (funny how this guy has access to every graveyard and mortuary in his town) and lurid (where would we be in a 70s exploitationer without our gratuitous lesbian scene?). Not to mention the \"\"romantic\"\" aspect (oh, how sweet!)...Miss it. (*)\",neg,10004_2.txt\r\n",
            "6,test,\"This German horror film has to be one of the weirdest I have seen.<br /><br />I was not aware of any connection between child abuse and vampirism, but this is supposed based upon a true character.<br /><br />Our hero is deaf and mute as a result of repeated beatings at the hands of his father. he also has a doll fetish, but I cannot figure out where that came from. His co-workers find out and tease him terribly.<br /><br />During the day a mild-manner accountant, and at night he breaks into cemeteries and funeral homes and drinks the blood of dead girls. They are all attractive, of course, else we wouldn't care about the fact that he usually tears their clothing down to the waist. He graduates eventually to actually killing, and that is what gets him caught.<br /><br />Like I said, a very strange movie that is dark and very slow as Werner Pochath never talks and just spends his time drinking blood.\",neg,10005_2.txt\r\n",
            "7,test,\"Being a long-time fan of Japanese film, I expected more than this. I can't really be bothered to write to much, as this movie is just so poor. The story might be the cutest romantic little something ever, pity I couldn't stand the awful acting, the mess they called pacing, and the standard \"\"quirky\"\" Japanese story. If you've noticed how many Japanese movies use characters, plots and twists that seem too \"\"different\"\", forcedly so, then steer clear of this movie. Seriously, a 12-year old could have told you how this movie was going to move along, and that's not a good thing in my book.<br /><br />Fans of \"\"Beat\"\" Takeshi: his part in this movie is not really more than a cameo, and unless you're a rabid fan, you don't need to suffer through this waste of film.<br /><br />2/10\",neg,10006_2.txt\r\n",
            "8,test,\"\"\"Tokyo Eyes\"\" tells of a 17 year old Japanese girl who falls in like with a man being hunted by her big bro who is a cop. This lame flick is about 50% filler and 50% talk, talk, and more talk. You'll get to see the less than stellar cast of three as they talk on the bus, talk and play video games, talk and get a haircut, talk and walk and walk and talk, talk on cell phones, hang out and talk, etc. as you read subtitles waiting for something to happen. The thin wisp of a story is not sufficient to support a film with low end production value, a meager cast, and no action, no romance, no sex or nudity, no heavy drama...just incessant yadayadayada'ing. (C-)\",neg,10007_4.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6LIcpjX-8up",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "8485e3af-17ee-462a-912d-1d31232a3c5b"
      },
      "source": [
        "data = pd.read_csv('imdb.csv')\n",
        "data.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  type  ... label         file\n",
              "0           0  test  ...   neg      0_2.txt\n",
              "1           1  test  ...   neg  10000_4.txt\n",
              "2           2  test  ...   neg  10001_1.txt\n",
              "3           3  test  ...   neg  10002_3.txt\n",
              "4           4  test  ...   neg  10003_3.txt\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XJh2SAb_NF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = data.drop([\"file\"], axis=1)\n",
        "data = data.drop([\"Unnamed: 0\"], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0g9yJ2GAbP6",
        "colab_type": "code",
        "outputId": "1089e3a6-c8b7-4227-9459-e0a7e43f32ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   type                                             review label\n",
              "0  test  Once again Mr. Costner has dragged out a movie...   neg\n",
              "1  test  This is an example of why the majority of acti...   neg\n",
              "2  test  First of all I hate those moronic rappers, who...   neg\n",
              "3  test  Not even the Beatles could write songs everyon...   neg\n",
              "4  test  Brass pictures (movies is not a fitting word f...   neg"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwc_EgVE_dXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = data.loc[data[\"type\"] == \"test\"].drop([\"type\"], axis=1)\n",
        "train_data = data.loc[data[\"type\"] == \"train\"].drop([\"type\"], axis=1)\n",
        "train_data = train_data.loc[train_data[\"label\"] != \"unsup\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tOMrYRXIq8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv(\"imdb_train.csv\", encoding=\"utf-8\")\n",
        "test_data.to_csv(\"imdb_test.csv\", encoding=\"utf-8\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bspb-Gj_w1DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenizer(text): # create a tokenizer function\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qo-HqVD0M7u",
        "colab_type": "code",
        "outputId": "cdbca266-10a1-4453-94f0-33a0fef1ae55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2pZufVu_x7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes={\n",
        "    'neg': 0,\n",
        "    'pos': 1\n",
        "}\n",
        "\n",
        "TEXT = Field(include_lengths=True, batch_first=True, \n",
        "             tokenize=tokenizer,\n",
        "             eos_token='<eos>',\n",
        "             lower=True,\n",
        "             stop_words=nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "LABEL = LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
        "\n",
        "train, test = TabularDataset.splits(path=\".\", train='imdb_train.csv', test=\"imdb_test.csv\", format='csv',\n",
        "               fields=[('id', None), ('review', TEXT), ('label', LABEL)], \n",
        "               skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUkG5g0wR3Yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEXT.build_vocab(train,\n",
        "                 max_size = 25000,\n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTx11QSiQ_-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state = np.random.seed(SEED)\n",
        "train, valid = train.split(0.7, stratified=True, random_state=state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjLsOZNuRmk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random, torch\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSKsxv7uQsKH",
        "colab_type": "text"
      },
      "source": [
        "#stuff"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfFT35ixw1DO",
        "colab_type": "code",
        "outputId": "c77a80e2-f24e-42c7-dc5d-37755f683e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "classes={\n",
        "    'neg':0,\n",
        "    'unsup':1,\n",
        "    'pos':2\n",
        "}\n",
        "\n",
        "TEXT = Field(include_lengths=True, batch_first=True, \n",
        "             tokenize=tokenizer,\n",
        "             eos_token='<eos>',\n",
        "             lower=True,\n",
        "             stop_words=nltk.corpus.stopwords.words('english'))\n",
        "LABEL = LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
        "\n",
        "dataset = TabularDataset('imdb_fixed.csv', format='csv', \n",
        "                         fields=[(None, None), ('type', RawField()), ('review', TEXT),('label', LABEL), (None, None)], \n",
        "                         skip_header=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-18e6ed904a8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m dataset = TabularDataset('imdb_fixed.csv', format='csv', \n\u001b[1;32m     15\u001b[0m                          \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRawField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                          skip_header=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, format, fields, skip_header, csv_reader_params, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py\u001b[0m in \u001b[0;36mfromCSV\u001b[0;34m(cls, data, fields, field_to_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfromCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_to_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfield_to_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/example.py\u001b[0m in \u001b[0;36mfromlist\u001b[0;34m(cls, data, fields)\u001b[0m\n\u001b[1;32m     50\u001b[0m                         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-3b2c149fed63>\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# create a tokenizer function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_en\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/lang/lex_attrs.py\u001b[0m in \u001b[0;36mlower\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9IGduGRw1DX",
        "colab_type": "code",
        "outputId": "8ff085de-7c83-4e96-ccb0-b22d044cdca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TEXT.build_vocab(dataset, min_freq=10, vectors=\"glove.6B.100d\")\n",
        "TEXT.build_vocab(dataset, min_freq=5)\n",
        "len(TEXT.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52275"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjW3k237w1Dh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL.build_vocab(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zAX1so6As0L",
        "colab_type": "code",
        "outputId": "511529f6-82db-4fa3-9e63-dbab8163f5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "LABEL.vocab.itos[:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mIVTqZPCKR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = Dataset(dataset.examples, dataset.fields, filter_pred= lambda x: x.type == 'train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9hVw8PsM5RM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = Dataset(dataset.examples, dataset.fields, filter_pred= lambda x: x.type == 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWU8C3HCWIzZ",
        "colab_type": "code",
        "outputId": "62a8e8da-48d2-4622-d903-d8afaa044c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "train, valid = train.split(0.7, stratified=True, random_state=np.random.seed(SEED))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-a08ebf6af524>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkD8DvZdw1Dr",
        "colab_type": "code",
        "outputId": "9ecb7f46-b108-4812-f4f8-eca574d3cf0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique([x.label for x in train.examples], return_counts=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([8750, 8750]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNCh-w3jaMWe",
        "colab_type": "code",
        "outputId": "425f938c-ff17-473a-88e6-f2d58464bdd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique([x.label for x in valid.examples], return_counts=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([3750, 3750]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bYvQwtuw1D2",
        "colab_type": "code",
        "outputId": "381b3fd7-ef49-42e4-bbe3-4de1a21ddb44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.unique([x.label for x in test.examples], return_counts=True)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0, 1]), array([12500, 12500]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK2KrgvQQ2Ha",
        "colab_type": "text"
      },
      "source": [
        "#continue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAAL8trKw1D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, kernels):\n",
        "\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embed_size, hidden_size, k, padding=5) for k in kernels])\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size * len(kernels), 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1,2)\n",
        "        \n",
        "        concatenated = []\n",
        "        for conv in self.convs:\n",
        "            z = conv(x)\n",
        "            z = F.avg_pool1d(z, kernel_size=z.size(2))\n",
        "            z = z.squeeze(2)\n",
        "            concatenated.append(z)\n",
        "            \n",
        "        x = tt.cat(concatenated, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocEWzsF6w1EE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt.cuda.empty_cache()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "model = MyModel(len(TEXT.vocab.itos),\n",
        "                embed_size=100,\n",
        "                hidden_size=128,\n",
        "                kernels=[2,3,4,5]\n",
        "               )\n",
        "\n",
        "train_loader, valid_loader, test_loader = BucketIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_sizes=(batch_size, batch_size, batch_size),\n",
        "    shuffle=True,\n",
        "    sort_key=lambda x: len(x.review),\n",
        "#     sort_within_batch=True,\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag4SY8xC55U8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_func(model, train_iterator, valid_iterator, criterion, optimizer, epochs):\n",
        "  best_valid_loss = 10.\n",
        "  acc_scores = []\n",
        "\n",
        "  for n_epoch in range(epochs):\n",
        "      \n",
        "      train_losses = []\n",
        "      valid_losses = []\n",
        "      valid_targets = []\n",
        "      valid_pred_class = []\n",
        "      \n",
        "      model.train()\n",
        "      \n",
        "      for xy in train_loader:\n",
        "\n",
        "          x = xy.review[0]\n",
        "          y = xy.label\n",
        "\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          pred = model(x)\n",
        "          loss = criterion(pred, y)\n",
        "          \n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer.step()\n",
        "          \n",
        "          train_losses.append(loss.item())\n",
        "\n",
        "      model.eval()\n",
        "      \n",
        "      for xy in valid_loader:\n",
        "\n",
        "          x = xy.review[0]\n",
        "          y = xy.label\n",
        "\n",
        "          x = x.to(device)\n",
        "\n",
        "          with torch.no_grad():\n",
        "\n",
        "              pred = model(x)\n",
        "\n",
        "              pred = pred.cpu()\n",
        "\n",
        "              valid_targets.append(y.numpy())\n",
        "              valid_pred_class.append(np.argmax(pred, axis=1))\n",
        "\n",
        "              loss = criterion(pred, y)\n",
        "\n",
        "              valid_losses.append(loss.item())\n",
        "          \n",
        "      mean_valid_loss = np.mean(valid_losses)\n",
        "\n",
        "      valid_targets = np.concatenate(valid_targets).squeeze()\n",
        "      valid_pred_class = np.concatenate(valid_pred_class).squeeze()\n",
        "\n",
        "      acc_score = metrics.accuracy_score(valid_targets, valid_pred_class)\n",
        "\n",
        "      acc_scores.append(acc_score)\n",
        "      \n",
        "      print('Losses: train - {:.3f}, test - {:.3f}'.format(np.mean(train_losses), mean_valid_loss))\n",
        "      print('Accuracy score - {:.3f}'.format(acc_score))\n",
        "          \n",
        "      # Early stopping:\n",
        "      if mean_valid_loss < best_valid_loss:\n",
        "          best_valid_loss = mean_valid_loss\n",
        "      else:\n",
        "          print('Early stopping')\n",
        "          break\n",
        "      return train_losses, acc_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnpUkck6kCrJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7ab96dcc-22f7-4247-c43b-77eacdbe972c"
      },
      "source": [
        "losses_1, acc_scores_1 = train_func(model, train_loader, valid_loader, criterion, optimizer, epochs=3)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses: train - 0.659, test - 0.540\n",
            "Accuracy score - 0.799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkCYGs_Rm7Bz",
        "colab_type": "text"
      },
      "source": [
        "Теперь посмотрим на результаты тестовой выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CTKFodnw1EM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_func(model, test_iterator):\n",
        "  model.eval()\n",
        "\n",
        "  test_targets = []\n",
        "  test_pred_class = []\n",
        "\n",
        "  for xy in test_iterator:\n",
        "      x = xy.review[0]\n",
        "      y = xy.label\n",
        "\n",
        "      x = x.to(device)\n",
        "\n",
        "      with tt.no_grad():\n",
        "\n",
        "          pred = model(x)\n",
        "\n",
        "          pred = pred.cpu()\n",
        "\n",
        "          test_targets.append(y.numpy())\n",
        "          test_pred_class.append(np.argmax(pred, axis=1))\n",
        "\n",
        "  test_targets = np.concatenate(test_targets).squeeze()\n",
        "  test_pred_class = np.concatenate(test_pred_class).squeeze()\n",
        "\n",
        "  acc = metrics.accuracy_score(test_targets, test_pred_class)\n",
        "\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRqPTh_xl4Zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2677120-8562-4ab5-eb78-bee4ba41eaa4"
      },
      "source": [
        "test_res_1 = test_func(model, test_loader)\n",
        "print(\"Test accuracy: \", np.mean(test_res_1))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.79112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtovJDj4imcY",
        "colab_type": "text"
      },
      "source": [
        "Добавим Dropout = 0.3, накинем на результат фильтров два линейных слоя, вместо одного с активационной функцией ReLU\n",
        "\n",
        "Также изменим следующие гиперпараметры: embed_size=300, hidden_size=150, \n",
        "kernels=[3,4,5], batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWhb5Z2dtCmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes={\n",
        "    'neg': 0,\n",
        "    'pos': 1\n",
        "}\n",
        "\n",
        "TEXT = Field(include_lengths=True, batch_first=True, \n",
        "             tokenize=tokenizer,\n",
        "             eos_token='<eos>',\n",
        "             lower=True,\n",
        "             stop_words=nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "LABEL = LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
        "\n",
        "train, test = TabularDataset.splits(path=\".\", train='imdb_train.csv', test=\"imdb_test.csv\", format='csv',\n",
        "               fields=[('id', None), ('review', TEXT), ('label', LABEL)], \n",
        "               skip_header=True)\n",
        "\n",
        "TEXT.build_vocab(train,\n",
        "                 max_size = 25000,\n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "state = np.random.seed(SEED)\n",
        "train, valid = train.split(0.7, stratified=True, random_state=state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08QDOR9ahqO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel_2(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, kernels, dropout_rate):\n",
        "\n",
        "        super(MyModel_2, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        self.out = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embed_size, hidden_size, k, padding=5) for k in kernels])\n",
        "        \n",
        "        self.linear_1 = torch.nn.Linear(hidden_size * len(kernels), round((hidden_size * len(kernels))/2))\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.linear_2 = torch.nn.Linear(round((hidden_size * len(kernels))/2), 2) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        x = self.out(x)\n",
        "        x = x.transpose(1,2)\n",
        "        \n",
        "        concatenated = []\n",
        "        for conv in self.convs:\n",
        "            z = conv(x)\n",
        "            z = F.avg_pool1d(z, kernel_size=z.size(2))\n",
        "            z = z.squeeze(2)\n",
        "            concatenated.append(z)\n",
        "            \n",
        "        x = tt.cat(concatenated, 1)\n",
        "\n",
        "        x = self.linear_1(x)\n",
        "        x = self.relu(x)    \n",
        "        x = self.linear_2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYLlayDmkvDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt.cuda.empty_cache()\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "model = MyModel_2(len(TEXT.vocab.itos),\n",
        "                embed_size=300,\n",
        "                hidden_size=150,\n",
        "                kernels=[3,4,5],\n",
        "                dropout_rate = 0.3\n",
        "                )\n",
        "\n",
        "train_loader, valid_loader, test_loader = BucketIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_sizes=(batch_size, batch_size, batch_size),\n",
        "    shuffle=True,\n",
        "    sort_key=lambda x: len(x.review),\n",
        "#     sort_within_batch=True,\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z94gdg4HlSor",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "51f5abad-426e-4ff3-fb88-68e4c40daaa7"
      },
      "source": [
        "losses_2, acc_scores_2 = train_func(model, train_loader, valid_loader, criterion, optimizer, epochs = 3)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses: train - 0.711, test - 0.546\n",
            "Accuracy score - 0.752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqDuFF_FqgYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fac21975-1bad-41e3-ae92-157aa4aeaa38"
      },
      "source": [
        "test_res_2 = test_func(model, test_loader)\n",
        "print(\"Test accuracy: \", np.mean(test_res_2))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.75564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cOS7G-xzMuV",
        "colab_type": "text"
      },
      "source": [
        "Результаты ухудшились попробуем уменьшить количество обучаемых признаков, но увеличить количество эпох. Дополнительный линейный слой тоже уберем, dropout оставим, но уменьшим"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WtaOtsOt75vm",
        "colab": {}
      },
      "source": [
        "class MyModel_3(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, kernels, dropout_rate):\n",
        "\n",
        "        super(MyModel_3, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        \n",
        "        self.out = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(embed_size, hidden_size, k, padding=5) for k in kernels])\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size * len(kernels), 3)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        x = self.out(x)\n",
        "        x = x.transpose(1,2)\n",
        "        \n",
        "        concatenated = []\n",
        "        for conv in self.convs:\n",
        "            z = conv(x)\n",
        "            z = F.avg_pool1d(z, kernel_size=z.size(2))\n",
        "            z = z.squeeze(2)\n",
        "            concatenated.append(z)\n",
        "            \n",
        "        x = tt.cat(concatenated, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gvxywrA2IY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes={\n",
        "    'neg': 0,\n",
        "    'pos': 1\n",
        "}\n",
        "\n",
        "TEXT = Field(include_lengths=True, batch_first=True, \n",
        "             tokenize=tokenizer,\n",
        "             eos_token='<eos>',\n",
        "             lower=True,\n",
        "             stop_words=nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "LABEL = LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
        "\n",
        "train, test = TabularDataset.splits(path=\".\", train='imdb_train.csv', test=\"imdb_test.csv\", format='csv',\n",
        "               fields=[('id', None), ('review', TEXT), ('label', LABEL)], \n",
        "               skip_header=True)\n",
        "\n",
        "TEXT.build_vocab(train,\n",
        "                 max_size = 25000,\n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "state = np.random.seed(SEED)\n",
        "train, valid = train.split(0.7, stratified=True, random_state=state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSrnT_ar2gNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt.cuda.empty_cache()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "model = MyModel_3(len(TEXT.vocab.itos),\n",
        "                embed_size=100,\n",
        "                hidden_size=128,\n",
        "                kernels=[2,3,4,5],\n",
        "                dropout_rate = 0.2\n",
        "                )\n",
        "\n",
        "train_loader, valid_loader, test_loader = BucketIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_sizes=(batch_size, batch_size, batch_size),\n",
        "    shuffle=True,\n",
        "    sort_key=lambda x: len(x.review),\n",
        "#     sort_within_batch=True,\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LChy3A12WvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dc240b86-f4ff-4334-b302-d33249be61c2"
      },
      "source": [
        "losses_4, acc_scores_4 = train_func(model, train_loader, valid_loader, criterion, optimizer, epochs = 10)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses: train - 0.667, test - 0.514\n",
            "Accuracy score - 0.791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO9hq0252vKz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7aa99f7-b458-40f4-8e69-b0fd96761050"
      },
      "source": [
        "test_res_4 = test_func(model, test_loader)\n",
        "print(\"Test accuracy: \", np.mean(test_res_4))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.7844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp_G0DOC6VLK",
        "colab_type": "text"
      },
      "source": [
        "Результат стал лучше, но все равно ниже, чем у первой модели\n",
        "\n",
        "Теперь попробуем изменить ограничение у словаря, добавить еще один кернел = 6, увеличить размер эмбеддинга до 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhZ0PdWq2vGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes={\n",
        "    'neg': 0,\n",
        "    'pos': 1\n",
        "}\n",
        "\n",
        "TEXT = Field(include_lengths=True, batch_first=True, \n",
        "             tokenize=tokenizer,\n",
        "             eos_token='<eos>',\n",
        "             lower=True,\n",
        "             stop_words=nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "LABEL = LabelField(dtype=tt.int64, use_vocab=True, preprocessing=lambda x: classes[x])\n",
        "\n",
        "train, test = TabularDataset.splits(path=\".\", train='imdb_train.csv', test=\"imdb_test.csv\", format='csv',\n",
        "               fields=[('id', None), ('review', TEXT), ('label', LABEL)], \n",
        "               skip_header=True)\n",
        "\n",
        "TEXT.build_vocab(train,\n",
        "                 min_freq=10,\n",
        "                 vectors=\"glove.6B.100d\")\n",
        "\n",
        "LABEL.build_vocab(train)\n",
        "\n",
        "state = np.random.seed(SEED)\n",
        "train, valid = train.split(0.7, stratified=True, random_state=state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWIiycF86NWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tt.cuda.empty_cache()\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "model = MyModel_3(len(TEXT.vocab.itos),\n",
        "                embed_size=200,\n",
        "                hidden_size=128,\n",
        "                kernels=[2,3,4,5,6],\n",
        "                dropout_rate = 0.2\n",
        "                )\n",
        "\n",
        "train_loader, valid_loader, test_loader = BucketIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_sizes=(batch_size, batch_size, batch_size),\n",
        "    shuffle=True,\n",
        "    sort_key=lambda x: len(x.review),\n",
        "#     sort_within_batch=True,\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True, cooldown=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuL1ywqe7INZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "92aacfa5-a91f-4639-e630-35b41eb0e56e"
      },
      "source": [
        "losses_5, acc_scores_5 = train_func(model, train_loader, valid_loader, criterion, optimizer, epochs = 10)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses: train - 0.660, test - 0.474\n",
            "Accuracy score - 0.824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "audPo-DZ7QXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f64220f-dcdb-4c94-b1e6-241fec53d871"
      },
      "source": [
        "test_res_5 = test_func(model, test_loader)\n",
        "print(\"Test accuracy: \", np.mean(test_res_5))"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy:  0.81564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHjvIfWPChNZ",
        "colab_type": "text"
      },
      "source": [
        "Данный результат - самый лучший из всех полученных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MupGLESZCnF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
