{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "assignment_7_mtransf.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WqyIfAxKRiv",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "Use following tokenization for english:  \n",
        "```\n",
        "import sentencepiece as spm\n",
        "\n",
        "...\n",
        "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "...\n",
        "TGT.build_vocab(..., min_freq=5)\n",
        "...\n",
        "\n",
        "```\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTlCrIAFMa7G",
        "colab_type": "code",
        "outputId": "645c3142-d20a-4580-9d2f-d153a6fe1cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 4.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 92kB 6.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 102kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 112kB 4.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 122kB 4.7MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 143kB 4.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 153kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 163kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 174kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 184kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 194kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 204kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 215kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 225kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 235kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 245kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 256kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 266kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 276kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 286kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 296kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 307kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 317kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 327kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 337kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 348kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 358kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 368kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 378kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 389kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 399kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 409kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 419kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 430kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 440kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 450kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 460kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 471kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 481kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 491kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 501kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 512kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 522kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 532kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 542kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 552kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 563kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 573kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 583kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 593kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 604kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 614kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 624kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 634kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 645kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 655kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 665kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 675kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 686kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 696kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 706kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 716kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 727kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 737kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 747kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 757kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 768kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 778kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 788kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 798kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 808kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 819kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 829kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 839kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 849kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 860kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 870kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 880kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 890kB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 901kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 911kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 921kB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 931kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 942kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 952kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 962kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 972kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 983kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 993kB 4.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.0MB 4.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.0MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyfVbvydKRjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from torchtext import datasets, data\n",
        "from tqdm import tqdm\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "import math, copy, time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGOZonI_UlQu",
        "colab_type": "code",
        "outputId": "7788eae1-3aec-4c31-9a57-8c6441cd5ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3E_CzPJXfCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('gdrive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mTbktTFKRjX",
        "colab_type": "code",
        "outputId": "b35734f9-2594-4717-a104-277c33bca53d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenize english \n",
        "with open('news-commentary-v13.ru-en.en') as f:\n",
        "    with open('text.en', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v0FW4ubhKRjg",
        "colab_type": "code",
        "outputId": "d163bcf4-1c9a-40e5-fdaf-e58884aa41aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# tokenize russian\n",
        "\n",
        "with open('news-commentary-v13.ru-en.ru') as f:\n",
        "    with open('text.ru', 'w') as out:\n",
        "            out.write(f.read().lower())\n",
        "        \n",
        "spm.SentencePieceTrainer.Train('--input=text.ru --model_prefix=bpe_ru --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWpw1Z7HKRjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load('bpe_ru.model')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "SRC = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', SRC), ('tgt', TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Coku1a0dKRkE",
        "colab_type": "code",
        "outputId": "dc0d41dd-b198-4173-c33a-45e218aca618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open('text.ru') as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open('text.en') as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235159it [01:15, 3100.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRVUITxVKRkh",
        "colab_type": "code",
        "outputId": "df1af615-1ea9-4518-a289-20e2be213d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('src: ' + \" \".join(train.examples[50].src))\n",
        "print('tgt: ' + \" \".join(train.examples[50].tgt))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ▁высокомерие ▁приводит ▁к ▁возникновению ▁ « бе лых ▁пят ен » .\n",
            "tgt: ▁hubris ▁creates ▁blind ▁spots .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2EZEYneKRkz",
        "colab_type": "code",
        "outputId": "139279ec-aaf9-4c4e-ef42-580cc66aaf69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train), len(valid), len(test)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210743, 23416, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8FhZQAHKRlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(train, min_freq=5)\n",
        "SRC.build_vocab(train, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud4GK9jDhCHb",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdCTfBgGj5eY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF5bzBrQkC7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNT0Viuxoh5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlVo64gNonfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu9QkgyOowQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D4ZuztVo23F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Ydpw4-o7kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJdnPPnXo9Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqqvACkcpBol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqPDmDuppFr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYcAMuJfpTkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ_JQ6zGpyLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8ep4YMNp7Z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9JfDIEeqHRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWhPmGo4qLeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US56Kd5GqYyQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=1, \n",
        "               d_model=512, d_ff=512, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3pdbt7Lq1yR",
        "colab_type": "text"
      },
      "source": [
        "# Batches and masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecxzvpaxqmJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9fHTRUgq7oj",
        "colab_type": "text"
      },
      "source": [
        "# Iterator and criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jik3a0HiKRlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "#         super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = 1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(\n",
        "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
        "            self.batch_sampler.__iter__()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "    \n",
        "class MyCriterion(nn.Module):\n",
        "    def __init__(self, generator, pad_idx):\n",
        "        super(MyCriterion, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.pad_idx = pad_idx\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        self.criterion.cuda()\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        ntokens = (target != self.pad_idx).data.sum()\n",
        "        x = self.generator(x)\n",
        "\n",
        "        return self.criterion(x.reshape(-1, x.size(-1)), \n",
        "                              target.reshape(-1))  / ntokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdH7huOLrhY2",
        "colab_type": "text"
      },
      "source": [
        "# Continue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcdjLydIKRlZ",
        "colab_type": "code",
        "outputId": "ed993ac8-2f7f-4593-816e-a24eafaa97ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "num_epochs = 1\n",
        "pad_idx = SRC.vocab.stoi[\"<pad>\"]\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits((train, valid, test), \n",
        "                                              batch_sizes=(batch_size, batch_size, batch_size), \n",
        "                                  sort_key=lambda x: len(x.src),\n",
        "                                  shuffle=True,\n",
        "                                  device=DEVICE,\n",
        "                                  sort_within_batch=False)\n",
        "                                  \n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)\n",
        "\n",
        "src_len = len(SRC.vocab)\n",
        "tgt_len = len(TGT.vocab)\n",
        "model = make_model(src_len, tgt_len, N=1)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "criterion = MyCriterion(model.generator, pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n",
        "\n",
        "# share weights\n",
        "#<TODO>"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqTmtLKsKRll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(data_iter, train_len, model, criterion, optimizer):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter, total=train_len)\n",
        "    counter = 0\n",
        "\n",
        "    for batch in data_iter:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "        loss = criterion(pred, batch.trg_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        curr_loss = loss.detach().item()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        data_iter.set_postfix(loss = curr_loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "def valid_epoch(data_iter, valid_len, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter, total=valid_len)\n",
        "    counter = 0\n",
        "\n",
        "    for batch in data_iter:\n",
        "        pred = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
        "        loss = criterion(pred, batch.trg_y)\n",
        "        \n",
        "        curr_loss = loss.detach().item()\n",
        "        \n",
        "        total_loss += curr_loss\n",
        "        data_iter.set_postfix(loss = curr_loss)\n",
        "        counter +=1\n",
        "        \n",
        "    total_loss /= counter\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nKOpdVH3xol",
        "colab_type": "code",
        "outputId": "8dfac1bb-96b4-4146-c7d1-235fc30deea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_len = len(list(iter(train_iter)))\n",
        "valid_len = len(list(iter(valid_iter)))\n",
        "\n",
        "train_len, valid_len"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1647, 183)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK9FbV5_zsSL",
        "colab_type": "code",
        "outputId": "210db3b6-b295-4c54-bef5-f5339b377e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train_epoch(train_iter, train_len, model, criterion, optimizer)\n",
        "    print('train', loss)\n",
        "    \n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = valid_epoch(valid_iter, valid_len, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print('valid', loss)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [8:32:11<00:00, 15.81s/it, loss=5.77]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train 6.071489456862338\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [15:47<00:00,  5.08s/it, loss=6.22]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid 5.646349437901231\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHtG8j3Ur7Fj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "86ee96f7-a8de-4fd9-8ab9-89905e032d54"
      },
      "source": [
        "#from sklearn.externals import joblib\n",
        "#joblib.dump(model, \"gen_model.pkl\")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gen_model.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGPHdzFxsAHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c1e08b86-59dc-4797-fe18-63baf27ba61b"
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "model = joblib.load(\"gen_model.pkl\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHzy7kljKRl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(model, src, src_mask, max_len=10, k=5):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(TGT.vocab.stoi[\"<s>\"]).type_as(src.data)\n",
        "    beam = [(ys, 0)]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        candidates= []\n",
        "        candidates_probs = []\n",
        "        prev_prob = None\n",
        "\n",
        "        for sent_pred, sent_prob in beam:\n",
        "            if sent_pred[0][-1] == '</s>':\n",
        "                candidates.append(send_pred)\n",
        "                candidates_probs.append(sent_prob)\n",
        "            else:\n",
        "                pr = model.decode(memory, src_mask, sent_pred,\n",
        "                                        subsequent_mask(sent_pred.size(1)).type_as(src.data))[0][i]\n",
        "                top_k = torch.argsort(-pr)[:k].tolist()\n",
        "                prev_prob = pr.tolist()\n",
        "                for t in top_k:\n",
        "                  candidates.append(torch.cat([sent_pred, torch.ones(1, 1).type_as(src.data).fill_(t)], dim=1))\n",
        "                  candidates_probs.append(sent_prob + np.log(pr.tolist()[t])) \n",
        "         \n",
        "        top_candidates = np.argsort(-np.array(candidates_probs))[:k]\n",
        "        beam = []\n",
        "        for cand in top_candidates:\n",
        "          beam.append((candidates[cand], candidates_probs[cand]))\n",
        "\n",
        "    return beam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "C6aj3hiNKRmG",
        "colab_type": "code",
        "outputId": "7fe8af7d-4b05-4a2f-b26d-5d099914fa19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(valid_iter):\n",
        "        src = batch.src[:1]\n",
        "        src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "        beam = beam_search(model, src, src_key_padding_mask)\n",
        "        \n",
        "        seq = []\n",
        "        for i in range(1, src.size(1)):\n",
        "            sym = SRC.vocab.itos[src[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_ru.decode_pieces(seq)\n",
        "        print(\"\\nSource:\", seq)\n",
        "        \n",
        "        print(\"Translation:\")\n",
        "        for pred, pred_proba in beam:                \n",
        "            seq = []\n",
        "            for i in range(1, pred.size(1)):\n",
        "                sym = TGT.vocab.itos[pred[0, i]]\n",
        "                if sym == \"</s>\": break\n",
        "                seq.append(sym)\n",
        "            seq = tok_en.decode_pieces(seq)\n",
        "            print(f\"pred {pred_proba:.2f}:\", seq)\n",
        "                \n",
        "        seq = []\n",
        "        for i in range(1, batch.trg.size(1)):\n",
        "            sym = TGT.vocab.itos[batch.trg[0, i]]\n",
        "            if sym == \"</s>\": break\n",
        "            seq.append(sym)\n",
        "        seq = tok_en.decode_pieces(seq)\n",
        "        print(\"Target:\", seq)\n",
        "        break"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Source: рост\n",
            "Translation:\n",
            "pred -7.44: between right there technology recogni see millionzz power\n",
            "pred -7.71: between right there technology recogni see millionz between right\n",
            "pred -7.78: between right there technology recogni see millionz power technology\n",
            "pred -7.85: between right there technology recogni see millionzzz\n",
            "pred -7.95: between right there technology recogni see millionz power point\n",
            "Target: inflation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXOqXr-VfX0f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e67f16c5-e73e-4fc8-aefb-50ee2b90e60f"
      },
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_iter):\n",
        "        for sent in range(len(batch.src)):\n",
        "            src = batch.src[sent:sent+1]\n",
        "            src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "            beam = beam_search(model, src, src_key_padding_mask, max_len = 20)\n",
        "\n",
        "            for pred, pred_proba in beam[:1]:                \n",
        "                seq = []\n",
        "                for i in range(1, pred.size(1)):\n",
        "                    sym = TGT.vocab.itos[pred[0, i]]\n",
        "                    if sym == \"</s>\":\n",
        "                      break\n",
        "                    seq.append(sym)\n",
        "                seq = tok_en.decode_pieces(seq)\n",
        "\n",
        "                new_trg = batch.trg[sent:sent+1].tolist()[0]\n",
        "                refs = []\n",
        "                for i in range(1, batch.trg.size(1)):\n",
        "                     new_trg_2 =  TGT.vocab.itos[new_trg[i]]\n",
        "                     if new_trg_2 == \"</s>\" or new_trg_2  == \"<pad>\":\n",
        "                       break\n",
        "                     refs.append(new_trg_2)\n",
        "                refs = tok_en.decode_pieces(refs)\n",
        "\n",
        "                hypotheses.append(seq.split())\n",
        "                references.append(refs.split())"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            " 12%|█▎        | 1/8 [01:14<08:38, 74.07s/it]\u001b[A\n",
            " 25%|██▌       | 2/8 [02:28<07:24, 74.15s/it]\u001b[A\n",
            " 38%|███▊      | 3/8 [03:41<06:08, 73.72s/it]\u001b[A\n",
            " 50%|█████     | 4/8 [04:54<04:54, 73.61s/it]\u001b[A\n",
            " 62%|██████▎   | 5/8 [06:07<03:40, 73.41s/it]\u001b[A\n",
            " 75%|███████▌  | 6/8 [07:23<02:28, 74.13s/it]\u001b[A\n",
            " 88%|████████▊ | 7/8 [08:37<01:14, 74.24s/it]\u001b[A\n",
            "100%|██████████| 8/8 [09:37<00:00, 69.98s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWChsjIDOZmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import translate\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3OQ7HWFKRmm",
        "colab_type": "code",
        "outputId": "0227e04f-2d50-4636-d04e-5777424a6b92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus_bleu(references, hypotheses, \n",
        "            smoothing_function=translate.bleu_score.SmoothingFunction().method3,\n",
        "            auto_reweigh=True\n",
        "           )"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.726114449927518e-05"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WaRgzfxOM13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}